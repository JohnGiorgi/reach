#
# Configuration file for reach
#

# this is the directory that stores the raw nxml files
# this directory *must* exist
nxmlDir = ${HOME}/Documents/nxml2fries/nxml

# this is where the output files containing the extracted mentions will be stored
# if this directory doesn't exist it will be created
friesDir = ${HOME}/Documents/nxml2fries/output

# this is where the context files will be stored
# if this directory doesn't exist it will be created
contextDir = ${HOME}/Documents/nxml2fries/context

# this is where the brat standoff and text files are dumped
bratDir = ${HOME}/Documents/nxml2fries/brat

# the encoding of input and output files
encoding = "utf-8"

# nxml2fries configuration
nxml2fries {

  # this is a list of sections that we should ignore
  ignoreSections = ["references", "materials", "materials|methods", "methods", "supplementary-material"]

}

# context engine config
contextEngine {
    type = Policy4
    params = {
        bound = 3
    }
}

# the output format for mentions: text, fries, indexcard (default is 'text')
outputType = "text"

# this log file gets overwritten every time ReachCLI is executed
# so you should copy it if you want to keep it around
logFile = ${HOME}/Documents/nxml2fries/log.txt

# grounding configuration
grounding: {
  # List of AdHoc grounding files to insert, in order, into the grounding search sequence.
  # Each element of the list is a map of KB filename and optional meta info (not yet used):
  #   example: { kb: "adhoc.tsv", source: "NMZ at CMU" }
  adHocFiles: [
#     { kb: "adhoc.tsv", source: "NMZ at CMU" }
  ]
}

# number of simultaneous threads to use for parallelization
threadLimit = 2

# ReadPapers
ReadPapers.papersDir = src/test/resources/inputs/nxml/
ReadPapers.serializedPapers = mentions.ser

# settings for assembly
assembly {
  # assembly can be run directly over a directory of papers (see RunAssembly)
  # papers directory can contain both nxml and csv formats
  papers = march2016-eval/papers
  # assembly output is in the form of csv files
  #
  # Currently, two csv files are produced for each paper:
  # 1. assembly matching MITRE's (March 2016) requirements
  # 2. unconstrained
  #
  # Additionally, two output files are produced to show assembly across all papers:
  # 1. assembly matching MITRE's (March 2016) requirements
  # 2. unconstrained
  outFolder = march2016-eval/output
  # assembly can also be performed against a serialized dataset (see AssembleFromDataset)
  serializedDataset = march2016-eval/march2016-eval.ser
  # a relation corpus (json)
  corpusFile = annotations.json
  # aseembly relation classifier
  classifier {
    # the trained model file (for reading and writing)
    model = lr-precedence-relation-classifier.model
    # report of results
    results = results.tsv
  }
}
