#
# Configuration file for reach-assembly
#

# the encoding of input and output files
encoding = "utf-8"

# this is a list of sections that we should ignore
ignoreSections = ["references", "materials", "materials|methods", "methods", "supplementary-material"]

# context engine config
contextEngine {
    type = Policy4
    params = {
        bound = 3
    }
}

logging {
  # defines project-wide logging level
  loglevel = INFO
  # this log file gets overwritten every time ReachCLI is executed
  # so you should copy it if you want to keep it around
  logfile = ${HOME}/reach-assembly.log
}

# grounding configuration
grounding: {
  # List of AdHoc grounding files to insert, in order, into the grounding search sequence.
  # Each element of the list is a map of KB filename and optional meta info (not yet used):
  #   example: { kb: "adhoc.tsv", source: "NMZ at CMU" }
  adHocFiles: [
    { kb: "NER-Grounding-Override.tsv.gz", source: "MITRE/NMZ/BG feedback overrides" }
  ]

  # flag to turn off the influence of species on grounding
  overrideSpecies = true
}

# number of simultaneous threads to use for parallelization
threadLimit = 2

# settings for assembly
assembly {
  # assembly can be run directly over a directory of papers (see ReachCLI) set by the papersDir property
  #
  # assembly output (outDir) is in the form of json or tsv files:
  #
  # TSV-style output
  # Currently, two tsv files are produced for each paper:
  # 1. assembly matching MITRE's (March 2016) requirements
  # 2. unconstrained
  #
  # Additionally, two output files are produced to show assembly across all papers:
  # 1. assembly matching MITRE's (March 2016) requirements
  # 2. unconstrained

  # consider pairs of mentions within N sentences of one another
  windowSize = 1

  corpus {
    # mentions with these labels may form an annotation pair
    validLabels = ["ComplexEvent", "Binding"]
    # a relation corpus (json)
    corpusFile = "annotations.json"
  }

  # assembly relation classifier
  classifier {

    trainingFile = "https://github.com/myedibleenso/this-before-that/raw/master/annotations.json"

    # what algorithm to use?
    classifier = lr-l1

    # the trained model file (for reading and writing)
    model = src/main/resources/org/clulab/assembly/fbc.model # this is an lr-l1 model

    # report of results
    results = results.tsv
  }

  # serialized gold PrecedenceRelations for evaluation
  evalGold = evalGold.ser

  # serialized mentions prior to applying rule-based sieves
  evalMentions = evalMentions.ser

  # allow additional information output
  verbose = true
}
